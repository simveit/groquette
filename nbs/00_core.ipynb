{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groquette\n",
    "\n",
    "> Simple library to work easily with Groq. Heavily inspired by [Claudette](https://github.com/AnswerDotAI/claudette/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect, typing, mimetypes, base64, json\n",
    "from collections import abc\n",
    "try: from IPython import display\n",
    "except: display=None\n",
    "\n",
    "from groq import Groq\n",
    "from groq.types.completion_usage import CompletionUsage\n",
    "from groq.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall\n",
    "from groq.types.chat import ChatCompletion\n",
    "from groq.types.chat.chat_completion import Choice\n",
    "from groq.types.chat.chat_completion_message import ChatCompletionMessage\n",
    "\n",
    "import toolslm\n",
    "from toolslm.funccall import *\n",
    "\n",
    "from fastcore import imghdr\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groq has available a bunch of models. To be able to select one of them in the future we make them accessible here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "models = \"llama3-70b-8192\", \"mixtral-8x7b-32768\", \"llama3-8b-8192\", \"gemma-7b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing a model is easy as that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama3-70b-8192', 'mixtral-8x7b-32768', 'llama3-8b-8192', 'gemma-7b-it')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will use the large Llama version as the most capable one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama3-70b-8192'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models[0]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of how to invoke the chat client, pass a message to it and get a reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-cab0945f-ec81-4b8f-8221-7824ea30f3cc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello Simon! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", role='assistant', function_call=None, tool_calls=None))], created=1719508342, model='llama3-70b-8192', object='chat.completion', system_fingerprint='fp_753a4aecf6', usage=CompletionUsage(completion_tokens=27, prompt_tokens=16, total_tokens=43, completion_time=0.077142857, prompt_time=0.007626808, queue_time=None, total_time=0.084769665), x_groq={'id': 'req_01j1day42jf249jhjaqdh65hmp'})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {\"role\":\"user\", \"content\":\"Hello, I am Simon.\"}\n",
    "r = cli.chat.completions.create(messages=[m], model=model, max_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello Simon! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", role='assistant', function_call=None, tool_calls=None))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we recieve a ChatCompletion object as a response. \n",
    "The ChatCompletion object includes information as the id, the choices (in our case exactly one choice) and other metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement a simple model to extract what we care about: The message we recieved from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def find_message(r:abc.Mapping, # The message to look in\n",
    "              ):\n",
    "    \"Finds the first message\"\n",
    "    return first(c.message for c in r.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=\"Hello Simon! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_message(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a ChatCompletionMessage has different attributes: \n",
    "* content\n",
    "* role\n",
    "* function_call\n",
    "* tool_calls\n",
    "\n",
    "Let's write a function that extracts only one of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_message_attribute(response, attribute='content'):\n",
    "    \"Return specified attribute of the message in the response.\"\n",
    "    msg = find_message(response)\n",
    "    return getattr(msg, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Simon! It's nice to meet you. Is there something I can help you with, or would you like to chat?\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_message_attribute(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response also contains a field with usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=27, prompt_tokens=16, total_tokens=43, completion_time=0.077142857, prompt_time=0.007626808, queue_time=None, total_time=0.084769665)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def usage(inp=0, # Number of input tokens\n",
    "          out=0  # Number of output tokens\n",
    "         ):\n",
    "    \"Slightly more concise version of `CompletionUsage`.\"\n",
    "    return CompletionUsage(prompt_tokens=inp, completion_tokens=out, total_tokens=inp+out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=8, prompt_tokens=5, total_tokens=13, completion_time=None, prompt_time=None, queue_time=None, total_time=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a total function to get the number of total tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch(as_prop=True)\n",
    "def total(self:CompletionUsage): return self.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5,8).total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __repr__(self:CompletionUsage): return f'In: {self.prompt_tokens}; Out: {self.completion_tokens}; Total: {self.total}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__repr__` function let's us get a convenient displaying of the usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 5; Out: 8; Total: 13"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __add__(self:CompletionUsage, b):\n",
    "    \"Add together each of `input_tokens` and `output_tokens`\"\n",
    "    return usage(self.prompt_tokens+b.prompt_tokens, self.completion_tokens+b.completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding two usages together can be accomplished by implementing `__add__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 8; Out: 10; Total: 18"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(3,4) + usage(5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
